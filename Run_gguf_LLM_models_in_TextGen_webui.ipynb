{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# $\\color{#19ABEA}{\\text{Run gguf LLM models in Latest Version TextGen-webui :}}$"
      ],
      "metadata": {
        "id": "KW0lK7R8yp_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_bLc7-6yVQS"
      },
      "outputs": [],
      "source": [
        "#@title # $\\color{#19ABEA}{\\text{Download and install requirements}}$ {display-mode: \"form\"}\n",
        "#@markdown $\\color{#19ABEA}{\\text{run this only once}}$\n",
        "\n",
        "#@markdown `Note :no need to check these checkboxes unless u facing a problem`\n",
        "#only if u want to remuve existing folder#!rm -rf /content/text-generation-webui\n",
        "%cd /content\n",
        "!git clone -b snapshot-2023-12-24 https://github.com/oobabooga/text-generation-webui\n",
        "# !git clone -b V20231127 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/UWUplus/flask-cloudflared# the origin\n",
        "!pip install -r extensions/openai/requirements.txt --upgrade\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "reinstall_llama_cpp_python = False # @param {type:\"boolean\"}\n",
        "if reinstall_llama_cpp_python :\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "#@markdown  `reinstalling flash attentionv2 may be needed  for some models like phi2..`\n",
        "reinstall_flash_attention = False # @param {type:\"boolean\"}\n",
        "if reinstall_flash_attention:\n",
        "  !pip uninstall -y flash-attn\n",
        "  !pip install --no-build-isolation flash-attn==2.3.0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # $\\color{#19ABEA}{define funcs}$ {display-mode: \"form\"}\n",
        "#@markdown  ________\n",
        "#@markdown  ### run this only once\n",
        "#@markdown  ________\n",
        "#==================================================================\n",
        "#==================================================================\n",
        "#==================================================================\n",
        "def downlink_to_dict(models_only=[]):\n",
        "  if len(models_only) > 0 :\n",
        "    dic ={}\n",
        "    for model_link in models_only:\n",
        "        splt =model_link.split(\".\")\n",
        "        quantization_name = splt[len(splt)-2]\n",
        "        dic[quantization_name] = model_link\n",
        "    return dic\n",
        "  return -1\n",
        "def get_donwloadlink2(repolink_tree_main,quant=None):\n",
        "    html = requests.get(repolink_tree_main)\n",
        "    soup=bs(html.text, 'html.parser')\n",
        "    all_downloads_link_ofcurrent_repo= soup.select(\"body > div.flex.min-h-screen.flex-col > main > div.container.relative.flex.flex-col.md\\:grid.md\\:space-y-0.w-full.md\\:grid-cols-12.space-y-4.md\\:gap-6.mb-16 > section > div:nth-child(4) > ul > li > a.group.col-span-4.flex.items-center.justify-self-end.truncate.text-right.font-mono.text-\\[0\\.8rem\\].leading-6.text-gray-400.md\\:col-span-2.xl\\:pr-10\")\n",
        "    #filtering_doownload_link geeting only  models\n",
        "    if quant!=None :\n",
        "      models_only = [ i['href'] for i in all_downloads_link_ofcurrent_repo if '.gguf' in i['href'] and quant in i['href']]\n",
        "      if len(models_only) >0 :\n",
        "        return f\"https://huggingface.co{models_only[0]}\".replace(\"?download=true\",\"\")\n",
        "      else :\n",
        "        print(\"=========================================\")\n",
        "        print(\"The quantization choosed inot Available the only Quant Available is :\")\n",
        "        models_only = [ i['href'] for i in all_downloads_link_ofcurrent_repo if '.gguf' in i['href']]\n",
        "        print(\"=========================================\")\n",
        "        print(models_only)\n",
        "        return None\n",
        "    else :\n",
        "      models_only = [ i['href'] for i in all_downloads_link_ofcurrent_repo if '.gguf' in i['href']]\n",
        "      return models_only\n",
        "\n",
        "def get_filename(repo_name,dl_link):\n",
        "  if dl_link is not None :\n",
        "    return dl_link.replace(f\"https://huggingface.co/{repo_name}/resolve/main/\",\"\").replace(\"?download=true\",\"\")\n",
        "  else :\n",
        "    return None\n",
        "#===========================================================\n",
        "#===========================================================\n",
        "#===========================================================\n",
        "\n"
      ],
      "metadata": {
        "id": "u6wKkF6i0GGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #  $\\color{#19ABEA}{Download Model}$ {display-mode: \"form\"}\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "#@markdown ## $\\color{#19ABEA}{\\text{Copy the RepoName here }}$\n",
        "\n",
        "#@markdown  Example :`TheBloke/OpenHermes-2.5-neural-chat-7B-v3-1-7B-GGUF`\n",
        "repo_name = \"TheBloke/xDAN-L1-Chat-RL-v1-GGUF\" # @param [\"TheBloke/Mistral-7B-OpenOrca-GGUF\", \"TheBloke/zephyr-7B-alpha-GGUF\", \"TheBloke/zephyr-7B-beta-GGUF\", \"TheBloke/neural-chat-7B-v3-1-GGUF\", \"TheBloke/mistral-ft-optimized-1218-GGUF\", \"TheBloke/xDAN-L1-Chat-RL-v1-GGUF\", \"TheBloke/openchat-3.5-1210-starling-slerp-GGUF\", \"TheBloke/Loyal-Macaroni-Maid-7B-GGUF\", \"TheBloke/Valkyrie-V1-GGUF\", \"TheBloke/Sonya-7B-GGUF\", \"TheBloke/Starling-LM-7B-alpha-GGUF\", \"TheBloke/SOLAR-10.7B-v1.0-GGUF\"] {allow-input: true}\n",
        "\n",
        "#@markdown  in colab gpu (15Gvram) you can use :\n",
        "#@markdown  - 13b model quantized upto Q5_K_M(context up to 4K)\n",
        "#@markdown  - 7b model quantized upto Q8_0(context up to 16K)\n",
        "flags=\"?not-for-all-audiences=true\"\n",
        "repo_link= f\"https://huggingface.co/{repo_name}/tree/main{flags}\"\n",
        "# print(repo_link)\n",
        "quant =\"Q6_K\" #@param ['Q2_K', 'Q3_K_L', 'Q3_K_M', 'Q3_K_S', 'Q4_0', 'Q4_1', 'Q4_K_M', 'Q4_K_S', 'Q5_0', 'Q5_1', 'Q5_K_M', 'Q5_K_S', 'Q6_K', 'Q8_0']\n",
        "model_download_url =get_donwloadlink2(repo_link,quant=quant)\n",
        "# print(\"dllink :\" ,model_download_url)\n",
        "model_file_name =get_filename(repo_name,model_download_url)\n",
        "# print(\"filename : \",model_file_name)\n",
        "if model_download_url is not None and model_file_name is not None :\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M $model_download_url -d /content/text-generation-webui/models/ -o $model_file_name\n",
        "else :\n",
        "  print(\"======================\")\n",
        "  print(\"please check the repo name or availability or the quantization\")"
      ],
      "metadata": {
        "id": "9AsZFjNe0Y7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##  $\\color{#19ABEA}{RunOnly}$ {display-mode: \"form\"}\n",
        "# %cd /content/text-generation-webui\n",
        "#@markdown  $\\color{#19ABEA}{\\text{Enter ur Context size or let it Unckechet to use the default size}}$\n",
        "params =\" \"\n",
        "#@markdown _____\n",
        "api =True # @param {type:\"boolean\"}\n",
        "if api :params +=\" --api --public-api --listen \"\n",
        "api_only_mode =False # @param {type:\"boolean\"}\n",
        "if api_only_mode : params+=\" --nowebui \"\n",
        "#@markdown _____\n",
        "use_custom_context_len = True # @param {type:\"boolean\"}\n",
        "context_len=4096*4 #@param\n",
        "if use_custom_context_len : params += f\" --n_ctx  {context_len} \"\n",
        "#@markdown _____\n",
        "use_custom_params = True # @param {type:\"boolean\"}\n",
        "custom_params = \"\" # @param {type:\"string\"}\n",
        "if use_custom_params : params +=custom_params\n",
        "\n",
        "#@markdown _____\n",
        "# params\n",
        "!python server.py   --share  --n-gpu-layers 100000  \t--model $model_file_name   $params"
      ],
      "metadata": {
        "id": "8jBKRxrXVceJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}