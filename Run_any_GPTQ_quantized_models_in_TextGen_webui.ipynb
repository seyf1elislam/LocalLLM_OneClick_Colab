{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyf1elislam/LocalLLM_OneClick_Colab/blob/main/Run_GPTQ_and_fullprecision_LLM_models_in_TextGen_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW0lK7R8yp_i"
      },
      "source": [
        "# $\\color{#19ABEA}{\\text{Run GPTQ and full precision LLM models in Latest Version TextGen-webui :}}$\n",
        "\n",
        "\n",
        "this [Notebook](https://github.com/seyf1elislam/LocalLLM_OneClick_Colab) allow u run  any quantized GPTQ file from any Hugging face repository as long as it fits the colab Vram and ram.\n",
        "\n",
        ">follow this [githubRepository](https://github.com/seyf1elislam/LocalLLM_OneClick_Colab) to get the updates of this noteboook.\n",
        "\n",
        "> Created by :\n",
        "> ðŸ˜‡ github :   [@seyf1elislam](https://github.com/seyf1elislam)\n",
        "> ðŸ¤— Huggingface : [@seyf1elislam](https://huggingface.co/seyf1elislam)\n",
        "\n",
        "## where can i find GPTQ quantized models ?\n",
        "> you can check those ðŸ¤— huggingface repos :\n",
        "- [Thebloke](https://huggingface.co/TheBloke) #1 best GPTQ source ,contains hundreds of quantized models.\n",
        "- [using search](https://huggingface.co/models?sort=trending&search=GPTQ) here u can find all GPTQ files on  huggingface.\n",
        "\n",
        "## need good models to try ?\n",
        "> you can try those :  \n",
        "- [TheBloke/SOLAR-10.7B-v1.0-GPTQ](https://huggingface.co/TheBloke/SOLAR-10.7B-v1.0-GPTQ)\n",
        "- [TheBloke/Kunoichi-7B-GPTQ](https://huggingface.co/TheBloke/Kunoichi-7B-GPTQ)\n",
        "- [TheBloke/Starling-LM-7B-alpha-GPTQ](https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GPTQ)\n",
        "\n",
        "## some Tips ?\n",
        "> most mistral models goes with 8k context length if u want to use longer context u need to make sure the models support longer context.\n",
        "\n",
        "> if u want to run model higher then 13B (such as 20B,4x7b..) on colab u may need to reduce the offloaded gpu models to split the ram usage between gpu vram and system ram. (slower but it works ðŸ˜‰)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_bLc7-6yVQS"
      },
      "outputs": [],
      "source": [
        "#@title # $\\color{#19ABEA}{\\text{Download and install requirements}}$ {display-mode: \"form\"}\n",
        "#@markdown $\\color{#19ABEA}{\\text{run this only once at the start}}$\n",
        "\n",
        "## @markdown `Note :no need to check these checkboxes unless u facing a problem`\n",
        "#@markdown ---\n",
        "#@markdown `check this if u need to change the version only`\n",
        "delete_existing_texgen_webui_folder =False #@param {type:\"boolean\"}\n",
        "if delete_existing_texgen_webui_folder :\n",
        "  !rm -rf /content/text-generation-webui\n",
        "#@markdown ---\n",
        "#@markdown ### TextGen webui version :\n",
        "#@markdown `note : gemma models are supported in snapshot-2024-02-25 or newer`\n",
        "%cd /content\n",
        "branch = \"dev\" # @param [\"dev\", \"snapshot-2024-01-07\", \"snapshot-2024-01-14\", \"snapshot-2024-01-28\", \"snapshot-2024-02-11\", \"snapshot-2024-02-25\", \"snapshot-2024-03-03\", \"snapshot-2024-03-10\", \"snapshot-2024-03-17\", \"snapshot-2024-03-24\", \"snapshot-2024-03-31\", \"snapshot-2024-04-07\", \"snapshot-2024-04-14\", \"snapshot-2024-04-21\", \"snapshot-2024-04-28\"]\n",
        "branch = f\"-b {branch}\"\n",
        "#@markdown `to download the latest version of TextGen Webui use use_latest ,this will ignore the branch name.`\n",
        "use_latest =False #@param {type:\"boolean\"}\n",
        "\n",
        "if use_latest :\n",
        "  branch = \"\"\n",
        "!git clone {branch} https://github.com/oobabooga/text-generation-webui\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/UWUplus/flask-cloudflared\n",
        "!pip install -r extensions/openai/requirements.txt --upgrade\n",
        "!apt-get -y install -qq aria2\n",
        "#@markdown ----\n",
        "reinstall_llama_cpp_python = False # @param {type:\"boolean\"}\n",
        "if reinstall_llama_cpp_python :\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "\n",
        "reinstall_flash_attention = False # @param {type:\"boolean\"}\n",
        "if reinstall_flash_attention:\n",
        "  !pip uninstall -y flash-attn\n",
        "  !pip install --no-build-isolation flash-attn==2.3.0\n",
        "#@markdown ----\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AsZFjNe0Y7_"
      },
      "outputs": [],
      "source": [
        "#@title #  $\\color{#19ABEA}{Download Model}$ {display-mode: \"form\"}\n",
        "#@markdown ## $\\color{#19ABEA}{\\text{Copy the RepoName here }}$\n",
        "#@markdown you can use GPTQ repository model here such as\n",
        "#@markdown   :`TheBloke/mistral-ft-optimized-1218-GPTQ`\n",
        "#@markdown  or :`TheBloke/xDAN-L1-Chat-RL-v1-GPTQ` etc...\n",
        "repo_name = \"TheBloke/mistral-ft-optimized-1218-GPTQ\" # @param [\"TheBloke/Mistral-7B-OpenOrca-GPTQ\", \"TheBloke/zephyr-7B-alpha-GPTQ\", \"TheBloke/zephyr-7B-beta-GPTQ\", \"TheBloke/neural-chat-7B-v3-1-GPTQ\", \"TheBloke/mistral-ft-optimized-1218-GPTQ\", \"TheBloke/xDAN-L1-Chat-RL-v1-GPTQ\", \"TheBloke/openchat-3.5-1210-starling-slerp-GPTQ\", \"TheBloke/Loyal-Macaroni-Maid-7B-GPTQ\", \"TheBloke/Valkyrie-V1-GPTQ\", \"TheBloke/Sonya-7B-GPTQ\", \"TheBloke/Starling-LM-7B-alpha-GPTQ\", \"TheBloke/SOLAR-10.7B-v1.0-GPTQ\"] {allow-input: true}\n",
        "# repo_name = \"xDAN-AI/xDAN-L1-Chat-RL-v1\" # @param [\"teknium/OpenHermes-2.5-Mistral-7B\", \"xDAN-AI/xDAN-L1-Chat-RL-v1\"]  { allow-input: true}\n",
        "\n",
        "#@markdown  in colab gpu (15Gvram) you can use :\n",
        "#@markdown - `13b model GPTQ `\n",
        "#@markdown - `7b model GPTQ`\n",
        "#@markdown - you can use full precision 13b , 7b modeles but using flags like `--load-in-4bit` or `--load-in-8bit`  its a bit slower then quantized versions but works well\n",
        "\n",
        "repo_link =f\"https://huggingface.co/{repo_name}\"\n",
        "print(repo_link)\n",
        "%cd /content/text-generation-webui/models\n",
        "!git lfs install\n",
        "!git clone $repo_link\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jBKRxrXVceJ"
      },
      "outputs": [],
      "source": [
        "#@title ##  $\\color{#19ABEA}{RunOnly}$ {display-mode: \"form\"}\n",
        "# %cd /content/text-generation-webui\n",
        "#@markdown  $\\color{#19ABEA}{\\text{Enter ur Context size or let it Unckechet to use the default size}}$\n",
        "params =\" \"\n",
        "#@markdown _____\n",
        "api =True # @param {type:\"boolean\"}\n",
        "if api :params +=\" --api --public-api --listen \"\n",
        "api_only_mode =False # @param {type:\"boolean\"}\n",
        "if api_only_mode : params+=\" --nowebui \"\n",
        "#@markdown _____\n",
        "use_custom_context_len = False # @param {type:\"boolean\"}\n",
        "context_len=8192 #@param\n",
        "if use_custom_context_len : params += f\" --n_ctx  {context_len} \"\n",
        "#@markdown _____\n",
        "use_load_in_flag = False # @param {type:\"boolean\"}\n",
        "load_in = \"--load-in-8bit\" # @param [\"--load-in-8bit\",\"--load-in-4bit\"] {type:\"string\"}\n",
        "if use_load_in_flag: params +=f\" {load_in} \"\n",
        "#@markdown _____\n",
        "use_custom_params = False # @param {type:\"boolean\"}\n",
        "custom_params = \"\" # @param {type:\"string\"}\n",
        "if use_custom_params : params +=custom_params\n",
        "\n",
        "\n",
        "#@markdown _____\n",
        "# params\n",
        "%cd /content/text-generation-webui\n",
        "model_file_name =repo_name.split('/')[-1]\n",
        "!python server.py   --share  --n-gpu-layers 100000  \t--model $model_file_name   $params"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMnBLm9DLCEYHTLoKO5oA30",
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
