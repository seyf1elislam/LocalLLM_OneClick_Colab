{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/hGuQmSpta0A7Hrecsd0S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# $\\color{#19ABEA}{\\text{Run GPTQ and full precision LLM models in Latest Version TextGen-webui :}}$"
      ],
      "metadata": {
        "id": "KW0lK7R8yp_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_bLc7-6yVQS"
      },
      "outputs": [],
      "source": [
        "#@title # $\\color{#19ABEA}{\\text{Download and install requirements}}$ {display-mode: \"form\"}\n",
        "#@markdown $\\color{#19ABEA}{\\text{run this only once}}$\n",
        "\n",
        "#@markdown `Note :no need to check these checkboxes unless u facing a problem`\n",
        "#only if u want to remuve existing folder#!rm -rf /content/text-generation-webui\n",
        "%cd /content\n",
        "!git clone -b snapshot-2023-12-24 https://github.com/oobabooga/text-generation-webui\n",
        "# !git clone -b V20231127 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/UWUplus/flask-cloudflared# the origin\n",
        "!pip install -r extensions/openai/requirements.txt --upgrade\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "reinstall_llama_cpp_python = False # @param {type:\"boolean\"}\n",
        "if reinstall_llama_cpp_python :\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "\n",
        "#@markdown  `reinstalling flash attentionv2 may be needed  for some models like phi2..`\n",
        "reinstall_flash_attention = False # @param {type:\"boolean\"}\n",
        "if reinstall_flash_attention:\n",
        "  !pip uninstall -y flash-attn\n",
        "  !pip install --no-build-isolation flash-attn==2.3.0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #  $\\color{#19ABEA}{Download Model}$ {display-mode: \"form\"}\n",
        "#@markdown ## $\\color{#19ABEA}{\\text{Copy the RepoName here }}$\n",
        "#@markdown you can use Past GPTQ model or fullPrecision here such as\n",
        "#@markdown   :`teknium/OpenHermes-2.5-Mistral-7B`\n",
        "#@markdown  or :`TheBloke/xDAN-L1-Chat-RL-v1-GPTQ` etc...\n",
        "repo_name = \"TheBloke/xDAN-L1-Chat-RL-v1-GPTQ\" # @param [\"TheBloke/Mistral-7B-OpenOrca-GPTQ\", \"TheBloke/zephyr-7B-alpha-GPTQ\", \"TheBloke/zephyr-7B-beta-GPTQ\", \"TheBloke/neural-chat-7B-v3-1-GPTQ\", \"TheBloke/mistral-ft-optimized-1218-GPTQ\", \"TheBloke/xDAN-L1-Chat-RL-v1-GPTQ\", \"TheBloke/openchat-3.5-1210-starling-slerp-GPTQ\", \"TheBloke/Loyal-Macaroni-Maid-7B-GPTQ\", \"TheBloke/Valkyrie-V1-GPTQ\", \"TheBloke/Sonya-7B-GPTQ\", \"TheBloke/Starling-LM-7B-alpha-GPTQ\", \"TheBloke/SOLAR-10.7B-v1.0-GPTQ\"] {allow-input: true}\n",
        "# repo_name = \"xDAN-AI/xDAN-L1-Chat-RL-v1\" # @param [\"teknium/OpenHermes-2.5-Mistral-7B\", \"xDAN-AI/xDAN-L1-Chat-RL-v1\"]  { allow-input: true}\n",
        "\n",
        "#@markdown  in colab gpu (15Gvram) you can use :\n",
        "#@markdown - `13b model GPTQ`\n",
        "#@markdown - `7b model GPTQ`\n",
        "#@markdown - you can use full precision 13b , 7b modles but using flags like `--load-in-4bit` or `--load-in-8bit`  its a bit slower then quantized versions but works well\n",
        "repo_link =f\"https://huggingface.co/{repo_name}\"\n",
        "print(repo_link)\n",
        "%cd /content/text-generation-webui/models\n",
        "!git lfs install\n",
        "!git clone $repo_link\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "9AsZFjNe0Y7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##  $\\color{#19ABEA}{RunOnly}$ {display-mode: \"form\"}\n",
        "# %cd /content/text-generation-webui\n",
        "#@markdown  $\\color{#19ABEA}{\\text{Enter ur Context size or let it Unckechet to use the default size}}$\n",
        "params =\" \"\n",
        "#@markdown _____\n",
        "api =True # @param {type:\"boolean\"}\n",
        "if api :params +=\" --api --public-api --listen \"\n",
        "api_only_mode =False # @param {type:\"boolean\"}\n",
        "if api_only_mode : params+=\" --nowebui \"\n",
        "#@markdown _____\n",
        "use_custom_context_len = False # @param {type:\"boolean\"}\n",
        "context_len=4096*4 #@param\n",
        "if use_custom_context_len : params += f\" --n_ctx  {context_len} \"\n",
        "#@markdown _____\n",
        "use_load_in_flag = False # @param {type:\"boolean\"}\n",
        "load_in = \"--load-in-8bit\" # @param [\"--load-in-8bit\",\"--load-in-4bit\"] {type:\"string\"}\n",
        "if use_load_in_flag: params +=f\" {load_in} \"\n",
        "#@markdown _____\n",
        "use_custom_params = False # @param {type:\"boolean\"}\n",
        "custom_params = \"\" # @param {type:\"string\"}\n",
        "if use_custom_params : params +=custom_params\n",
        "\n",
        "\n",
        "#@markdown _____\n",
        "# params\n",
        "%cd /content/text-generation-webui\n",
        "model_file_name =repo_name.split('/')[-1]\n",
        "!python server.py   --share  --n-gpu-layers 100000  \t--model $model_file_name   $params"
      ],
      "metadata": {
        "id": "8jBKRxrXVceJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}