{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyf1elislam/LocalLLM_OneClick_Colab/blob/main/awesome_koboldcpp_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FCn5tmpn3UV"
      },
      "source": [
        "\n",
        "# $\\color{#19ABEA}{\\text{Run any gguf quantized models in Latest Version KoboldCpp on Colab :}}$\n",
        "## This is modified version  KoboldCpp Colab Notebook\n",
        "\n",
        "It's really easy to get started. Just press the two **Play** buttons below, and then connect to the **Cloudflare URL** shown at the end.\n",
        "You can select a model from the dropdown, or enter a **custom repo name** l (Example: `unsloth/Qwen3-8B-GGUF` with quant `Q5_k_m`)\n",
        "\n",
        "**Keep this page open and occationally check for captcha's so that your AI is not shut down**\n",
        "\n",
        "> Notebook rep : [Notebook github Repository](https://github.com/seyf1elislam/LocalLLM_OneClick_Colab).   \n",
        "> koboldcpp : [https://github.com/LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QNaj3u0jn3UW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "7d229306-6d3d-430c-a1ae-4cbb5f99b899"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
              "<audio autoplay=\"\" src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" loop controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
        "<audio autoplay=\"\" src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" loop controls>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  $\\color{#19ABEA}{Run- ALL}$"
      ],
      "metadata": {
        "id": "EeVQU4PoahAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # $\\color{#19ABEA}{\\text{Download and install requirements}}$ {display-mode: \"form\"}\n",
        "\n",
        "%cd /content\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiCMDlrVWZSm",
        "outputId": "0cebf238-82ff-41e9-91f7-eb278ba7c231",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading KoboldCpp, please wait...\n",
            "--2025-08-06 21:54:28--  https://kcpplinux.concedo.workers.dev/\n",
            "Resolving kcpplinux.concedo.workers.dev (kcpplinux.concedo.workers.dev)... 172.67.145.201, 104.21.71.155, 2606:4700:3036::ac43:91c9, ...\n",
            "Connecting to kcpplinux.concedo.workers.dev (kcpplinux.concedo.workers.dev)|172.67.145.201|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-linux-x64 [following]\n",
            "--2025-08-06 21:54:28--  https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-linux-x64\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/LostRuins/koboldcpp/releases/download/v1.97/koboldcpp-linux-x64 [following]\n",
            "--2025-08-06 21:54:28--  https://github.com/LostRuins/koboldcpp/releases/download/v1.97/koboldcpp-linux-x64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/614637892/e77bd8ac-e88b-47b6-8e87-f4c3ef06f543?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T22%3A49%3A35Z&rscd=attachment%3B+filename%3Dkoboldcpp-linux-x64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T21%3A49%3A32Z&ske=2025-08-06T22%3A49%3A35Z&sks=b&skv=2018-11-09&sig=wFQM8uAfOHLu9noSVlttE0CaDiTPaTam8y4Ji5whGOQ%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDUxNzU2OCwibmJmIjoxNzU0NTE3MjY4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.JotqDcqwD2CNYtKqb2g_EudEjZ7SAqmVILOGACGOlQo&response-content-disposition=attachment%3B%20filename%3Dkoboldcpp-linux-x64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-06 21:54:28--  https://release-assets.githubusercontent.com/github-production-release-asset/614637892/e77bd8ac-e88b-47b6-8e87-f4c3ef06f543?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T22%3A49%3A35Z&rscd=attachment%3B+filename%3Dkoboldcpp-linux-x64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T21%3A49%3A32Z&ske=2025-08-06T22%3A49%3A35Z&sks=b&skv=2018-11-09&sig=wFQM8uAfOHLu9noSVlttE0CaDiTPaTam8y4Ji5whGOQ%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDUxNzU2OCwibmJmIjoxNzU0NTE3MjY4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.JotqDcqwD2CNYtKqb2g_EudEjZ7SAqmVILOGACGOlQo&response-content-disposition=attachment%3B%20filename%3Dkoboldcpp-linux-x64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577633688 (551M) [application/octet-stream]\n",
            "Saving to: ‘dlfile.tmp’\n",
            "\n",
            "dlfile.tmp          100%[===================>] 550.87M   179MB/s    in 3.1s    \n",
            "\n",
            "2025-08-06 21:54:31 (179 MB/s) - ‘dlfile.tmp’ saved [577633688/577633688]\n",
            "\n",
            "Download Successful\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,918 kB]\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Fetched 2,307 kB in 1s (1,902 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Fetched 1,513 kB in 1s (1,805 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title #  $\\color{#19ABEA}{Download- Model}$ {display-mode: \"form\"}\n",
        "#@markdown ## $\\color{#19ABEA}{\\text{Copy the RepoName here :}}$\n",
        "\n",
        "# ------------------------------------------\n",
        "from huggingface_hub import HfFileSystem\n",
        "fs = HfFileSystem()\n",
        "\n",
        "def get_donwloadlink_and_filename(repo_name,quant='Q5_K_M'):\n",
        "  gguf_files = fs.glob(f\"{repo_name}/*.gguf\")\n",
        "  filtered_files = [file for file in gguf_files if (quant in file or quant.lower() in file.lower() or quant.upper() in file.upper())]\n",
        "\n",
        "  if len(filtered_files) ==0 : # quant not existed\n",
        "    print( \"=\" * 10 +\"\\navailable quants:\\n\",gguf_files)\n",
        "    return None,None\n",
        "  file_path= filtered_files[0]\n",
        "  *repo_name , file_name = file_path.split(\"/\");\n",
        "  repo_name='/'.join(repo_name)\n",
        "  return f\"https://huggingface.co/{repo_name}/resolve/main/{file_name}?download=true\" ,file_name\n",
        "\n",
        "# ------------------------------------------\n",
        "#@markdown  Model name\n",
        "#@markdown  Example :`bartowski/Meta-Llama-3.1-8B-Instruct-GGUFF`\n",
        "repo_name = \"unsloth/Qwen3-8B-GGUF\" # @param [\"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\",\"bartowski/gemma-2-9b-it-GGUF\",\"QuantFactory/Mistral-Nemo-Instruct-2407-GGUF\",\"bartowski/Mistral-Small-Instruct-2409-GGUF\",\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\"unsloth/Qwen3-30B-A3B-GGUF\",\"unsloth/gpt-oss-20b-GGUF\",\"unsloth/Qwen3-8B-GGUF\",\"unsloth/Qwen3-14B-GGUF\",\"unsloth/gemma-3-12b-it-GGUF\"] {\"allow-input\":true}\n",
        "\n",
        "# ------------------------------------------\n",
        "# ------------------------------------------\n",
        "# ------------------------------------------\n",
        "quant = \"Q6_K\" # @param [\"Q2_K\",\"Q3_K_L\",\"Q3_K_M\",\"Q3_K_S\",\"Q4_0\",\"Q4_1\",\"Q4_K_M\",\"Q4_K_S\",\"Q5_0\",\"Q5_1\",\"Q5_K_M\",\"Q5_K_S\",\"Q6_K\",\"Q8_0\"]\n",
        "model_download_url,model_file_name =get_donwloadlink_and_filename(repo_name,quant=quant)\n",
        "\n",
        "if model_download_url is not None and model_file_name is not None :\n",
        "  model_download_url=model_download_url.replace(\"?download=true\",\"\")\n",
        "  print(model_download_url)\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_download_url} -d /content/ -o {model_file_name}\n",
        "  # !aria2c -c -x 10 -o {model_file_name} --summary-interval=5 --download-result=default --auto-file-renaming=false --file-allocation=none {model_download_url}\n",
        "else :\n",
        "  print(\"======================\")\n",
        "  print(\"please check the repo name or check availability of the quantized file inside the repo\")\n",
        "  print(\"======================\")\n",
        "  print(\"======================\")\n",
        "  print(\"======================\")\n",
        "  raise RuntimeError(\"⚠️Quant doest exist in the repository\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoaixij_tGK9",
        "outputId": "0f7252d0-6614-4b44-d4ac-389f422f7645"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://huggingface.co/unsloth/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q6_K.gguf\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "6c611f|\u001b[1;32mOK\u001b[0m  |   150MiB/s|/content//Qwen3-8B-Q6_K.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8860a5-cb0b-4383-92e2-f8cf7d8a95a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "***\n",
            "Welcome to KoboldCpp - Version 1.97\n",
            "Downloading https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Attempting to start tunnel thread...\n",
            "Loading Chat Completions Adapter: /tmp/_MEIJljzC5/kcpp_adapters/AutoGuess.json\n",
            "Chat Completions Adapter Loaded\n",
            "System: Linux #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025 x86_64 x86_64\n",
            "Detected Available GPU Memory: 15360 MB\n",
            "Unable to determine available RAM\n",
            "Initializing dynamic library: koboldcpp_cublas.so\n",
            "Starting Cloudflare Tunnel for Linux, please wait...\n",
            "==========\n",
            "Namespace(admin=False, admindir='', adminpassword=None, analyze='', benchmark=None, blasbatchsize=512, blasthreads=1, chatcompletionsadapter='AutoGuess', cli=False, config=None, contextsize=16384, debugmode=0, defaultgenamt=512, draftamount=8, draftgpulayers=999, draftgpusplit=None, draftmodel='', embeddingsgpu=False, embeddingsmaxctx=0, embeddingsmodel='', enableguidance=False, exportconfig='', exporttemplate='', failsafe=False, flashattention=True, forceversion=0, foreground=False, gpulayers=99, highpriority=False, hordeconfig=None, hordegenlen=0, hordekey='', hordemaxctx=0, hordemodelname='', hordeworkername='', host='', ignoremissing=False, launch=False, lora=None, loramult=1.0, maingpu=-1, maxrequestsize=32, mmproj='', mmprojcpu=False, model=[], model_param='Qwen3-8B-Q6_K.gguf', moecpu=0, moeexperts=-1, multiplayer=False, multiuser=1, noavx2=False, noblas=False, nobostoken=False, nocertify=False, nofastforward=False, nommap=False, nomodel=False, noshift=False, onready='', overridekv='', overridetensors='', password=None, port=5001, port_param=5001, preloadstory='', prompt='', promptlimit=100, quantkv=0, quiet=True, remotetunnel=True, ropeconfig=[0.0, 10000.0], savedatafile='', sdclamped=0, sdclampedsoft=0, sdclipg='', sdclipl='', sdconfig=None, sdlora='', sdloramult=1.0, sdmodel='', sdnotile=False, sdphotomaker='', sdquant=False, sdt5xxl='', sdthreads=0, sdtiledvae=768, sdvae='', sdvaeauto=False, showgui=False, singleinstance=False, skiplauncher=False, smartcontext=False, ssl=None, tensor_split=None, threads=1, ttsgpu=False, ttsmaxlen=4096, ttsmodel='', ttsthreads=0, ttswavtokenizer='', unpack='', useclblast=None, usecpu=False, usecuda=['0', 'mmq'], usemlock=False, usemmap=False, useswa=False, usevulkan=None, version=False, visionmaxres=1024, websearch=True, whispermodel='')\n",
            "==========\n",
            "Loading Text Model: /content/Qwen3-8B-Q6_K.gguf\n",
            "\n",
            "The reported GGUF Arch is: qwen3\n",
            "Arch Category: 0\n",
            "\n",
            "---\n",
            "Identified as GGUF model.\n",
            "Attempting to Load...\n",
            "---\n",
            "Using automatic RoPE scaling for GGUF. If the model has custom RoPE settings, they'll be used directly instead!\n",
            "System Info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "CUDA MMQ: True\n",
            "---\n",
            "Initializing CUDA/HIP, please wait, the following step may take a few minutes (only for first launch)...\n",
            "---\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /content/Qwen3-8B-Q6_K.gguf (version GGUF V3 (latest))\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file size   = 6.26 GiB (6.56 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load:   - 151662 ('<|fim_pad|>')\n",
            "load:   - 151663 ('<|repo_name|>')\n",
            "load:   - 151664 ('<|file_sep|>')\n",
            "load: special tokens cache size = 26\n",
            "load: token to piece cache size = 0.9311 MB\n",
            "print_info: arch             = qwen3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 40960\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 36\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 12288\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 40960\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 8B\n",
            "print_info: model params     = 8.19 B\n",
            "print_info: general.name     = Qwen3-8B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 11 ','\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
            "load_tensors: relocated tensors: 1 of 399\n",
            "load_tensors: offloading 36 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 37/37 layers to GPU\n",
            "load_tensors:          CPU model buffer size =   486.86 MiB\n",
            "load_tensors:        CUDA0 model buffer size =  5921.78 MiB\n",
            "load_all_data: using async uploads for device CUDA0, buffer type CUDA0, backend CUDA0\n",
            ".......................................................................................\n",
            "Automatic RoPE Scaling: Using model internal value.\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 16512\n",
            "llama_context: n_ctx_per_seq = 16512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 1\n",
            "llama_context: kv_unified    = true\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (16512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 16640 (padded)\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  2340.00 MiB\n",
            "llama_kv_cache_unified: size = 2340.00 MiB ( 16640 cells,  36 layers,  1/1 seqs), K (f16): 1170.00 MiB, V (f16): 1170.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 3192\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "llama_context:      CUDA0 compute buffer size =   353.26 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    40.51 MiB\n",
            "llama_context: graph nodes  = 1267\n",
            "llama_context: graph splits = 2\n",
            "Threadpool set to 1 threads and 1 blasthreads...\n",
            "attach_threadpool: call\n",
            "\n",
            "This architecture has explicitly disabled the BOS token - if you need it, you must add it manually.\n",
            "Starting model warm up, please wait a moment...\n",
            "Load Text Model OK: True\n",
            "Chat completion heuristic: ChatML (Qwen 2.5 based)\n",
            "Embedded KoboldAI Lite loaded.\n",
            "Embedded API docs loaded.\n",
            "======\n",
            "Active Modules: TextGeneration WebSearchProxy\n",
            "Inactive Modules: ImageGeneration VoiceRecognition MultimodalVision MultimodalAudio NetworkMultiplayer ApiKeyPassword TextToSpeech VectorEmbeddings AdminControl\n",
            "Enabled APIs: KoboldCppApi OpenAiApi OllamaApi\n",
            "Your remote Kobold API can be found at https://examples-fisheries-trial-launched.trycloudflare.com/api\n",
            "Your remote OpenAI Compatible API can be found at https://examples-fisheries-trial-launched.trycloudflare.com/v1\n",
            "======\n",
            "Your remote tunnel is ready, please connect to https://examples-fisheries-trial-launched.trycloudflare.com\n",
            "\n",
            "[22:02:34] CtxLimit:135/16384, Amt:124/512, Init:0.00s, Process:0.02s (687.50T/s), Generate:5.19s (23.91T/s), Total:5.20s\n",
            "[22:03:13] CtxLimit:568/16384, Amt:512/512, Init:0.02s, Process:0.02s (2421.05T/s), Generate:21.64s (23.66T/s), Total:21.66s"
          ]
        }
      ],
      "source": [
        "#@title #  $\\color{#19ABEA}{Start- koboldCpp}$ {display-mode: \"form\"}\n",
        "Model = model_download_url\n",
        "#@markdown Note : change the offloaded layer only if the model cant fit in the gpu.\n",
        "\n",
        "#@markdown - for models like `qwen-30B-A3B` it fits only 31 layer in gpu and the rest in system ram,\n",
        "\n",
        "#@markdown - for   `gpt-oss-20b` model you can fully load it with `Q4_k_m` .\n",
        "\n",
        "Layers = \"99\" # @param [\"99\",\"35\",\"31\"] {\"allow-input\":true}\n",
        "ContextSize = \"16384\" # @param [\"4096\",\"8192\",\"12288\",\"16384\"] {\"allow-input\":true}\n",
        "FlashAttention = True #@param {type:\"boolean\"}\n",
        "Multiplayer = False #@param {type:\"boolean\"}\n",
        "FACommand = \"\"\n",
        "MPCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadVisionMMProjector = False #@param {type:\"boolean\"}\n",
        "Mmproj = \"https://huggingface.co/koboldcpp/mmproj/resolve/main/LLaMA3-8B_mmproj-Q4_1.gguf\" #@param [\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/mistral-7b-mmproj-v1.5-Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-7b-mmproj-v1.5-Q4_0.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/LLaMA3-8B_mmproj-Q4_1.gguf\"]{allow-input: true}\n",
        "VCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadImgModel = False #@param {type:\"boolean\"}\n",
        "ImgModel = \"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_ftuned_q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_ftuned_q4_0.gguf\"]{allow-input: true}\n",
        "SCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadSpeechModel = False #@param {type:\"boolean\"}\n",
        "SpeechModel = \"https://huggingface.co/koboldcpp/whisper/resolve/main/whisper-base.en-q5_1.bin\" #@param [\"https://huggingface.co/koboldcpp/whisper/resolve/main/whisper-base.en-q5_1.bin\"]{allow-input: true}\n",
        "WCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadTTSModel = False #@param {type:\"boolean\"}\n",
        "TTSModel = \"https://huggingface.co/koboldcpp/tts/resolve/main/OuteTTS-0.2-500M-Q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/tts/resolve/main/OuteTTS-0.2-500M-Q4_0.gguf\"]{allow-input: true}\n",
        "WavTokModel = \"https://huggingface.co/koboldcpp/tts/resolve/main/WavTokenizer-Large-75-Q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/tts/resolve/main/WavTokenizer-Large-75-Q4_0.gguf\"]{allow-input: true}\n",
        "TTSCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadEmbeddingsModel = False #@param {type:\"boolean\"}\n",
        "EmbeddingsModel = \"https://huggingface.co/yixuan-chia/snowflake-arctic-embed-s-GGUF/resolve/main/snowflake-arctic-embed-s-Q4_0.gguf\" #@param [\"https://huggingface.co/yixuan-chia/snowflake-arctic-embed-s-GGUF/resolve/main/snowflake-arctic-embed-s-Q4_0.gguf\"]{allow-input: true}\n",
        "ECommand = \"\"\n",
        "#@markdown <hr>\n",
        "#@markdown This enables saving stories directly to your google drive. You will have to grant permissions, and then you can access the saves from the \"KoboldCpp Server Storage\" option.\n",
        "AllowSaveToGoogleDrive = False #@param {type:\"boolean\"}\n",
        "SavGdriveCommand = \"\"\n",
        "#@markdown <hr>\n",
        "#@markdown Only select the following box if regular cloudflare tunnel fails to work. It will generate an inferior localtunnel tunnel, which you can use after entering a password.\n",
        "MakeLocalTunnelFallback = False #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\")\n",
        "\n",
        "if AllowSaveToGoogleDrive:\n",
        "  print(\"Attempting to request access to save to your google drive...\")\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "    import os, json\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "      raise RuntimeError(\"Google Drive mount failed. Please grant permissions and try again.\")\n",
        "    kcppdir = '/content/drive/MyDrive/koboldcpp_data'\n",
        "    os.makedirs(kcppdir, exist_ok=True)\n",
        "    savedatapath = os.path.join(kcppdir, \"koboldcpp_save_db.jsondb\")\n",
        "    if not os.path.exists(savedatapath):\n",
        "        settings_data = {}\n",
        "        with open(savedatapath, \"w\") as json_file:\n",
        "            json.dump(settings_data, json_file, indent=4)\n",
        "        print(f\"Created new koboldcpp_save_db.jsondb at {savedatapath}\")\n",
        "    else:\n",
        "        print(f\"Loading saved data at {savedatapath}\")\n",
        "    SavGdriveCommand = f\" --savedatafile {savedatapath}\"\n",
        "  except Exception as e:\n",
        "    print(f\"⚠️ Error: {e}\")\n",
        "    print(\"Please ensure you grant Google Drive permissions and try again.\")\n",
        "\n",
        "%cd /content\n",
        "if Mmproj and LoadVisionMMProjector:\n",
        "  VCommand = \"--mmproj vmodel.gguf\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if ImgModel and LoadImgModel:\n",
        "  SCommand = \"--sdmodel imodel.gguf --sdthreads 4 --sdquant --sdclamped\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if SpeechModel and LoadSpeechModel:\n",
        "  WCommand = \"--whispermodel wmodel.bin\"\n",
        "else:\n",
        "  WCommand = \"\"\n",
        "if TTSModel and WavTokModel and LoadTTSModel:\n",
        "  TTSCommand = \"--ttsmodel ttsmodel.bin --ttswavtokenizer ttswavtok.bin --ttsgpu\"\n",
        "else:\n",
        "  TTSCommand = \"\"\n",
        "if EmbeddingsModel and LoadEmbeddingsModel:\n",
        "  ECommand = \"--embeddingsmodel emodel.bin\"\n",
        "else:\n",
        "  ECommand = \"\"\n",
        "if FlashAttention:\n",
        "  FACommand = \"--flashattention\"\n",
        "else:\n",
        "  FACommand = \"\"\n",
        "if Multiplayer:\n",
        "  MPCommand = \"--multiplayer\"\n",
        "else:\n",
        "  MPCommand = \"\"\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "# !echo Downloading KoboldCpp, please wait...\n",
        "# !wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "# !test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "# !chmod +x ./koboldcpp_linux\n",
        "# !apt update\n",
        "# !apt install aria2 -y\n",
        "# simple fix for a common URL mistake\n",
        "# ---------------------------------------------------------------------------------------------\n",
        "if \"https://huggingface.co/\" in Model and \"/blob/main/\" in Model:\n",
        "  Model = Model.replace(\"/blob/main/\", \"/resolve/main/\")\n",
        "# !aria2c -x 10 -o {model_file_name} --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Model\n",
        "# download only if diffrent\n",
        "# !aria2c -c -x 10 -o {model_file_name} --summary-interval=5 --download-result=default --auto-file-renaming=false --file-allocation=none $Model\n",
        "if VCommand:\n",
        "  !aria2c -x 10 -o vmodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Mmproj\n",
        "if SCommand:\n",
        "  !aria2c -x 10 -o imodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $ImgModel\n",
        "if WCommand:\n",
        "  !aria2c -x 10 -o wmodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $SpeechModel\n",
        "if TTSCommand:\n",
        "  !aria2c -x 10 -o ttsmodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $TTSModel\n",
        "  !aria2c -x 10 -o ttswavtok.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $WavTokModel\n",
        "if ECommand:\n",
        "  !aria2c -x 10 -o emodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $EmbeddingsModel\n",
        "\n",
        "if MakeLocalTunnelFallback:\n",
        "  import urllib\n",
        "  print(\"Trying to use LocalTunnel as a fallback tunnel (not so good)...\")\n",
        "  ltpw = urllib.request.urlopen('https://loca.lt/mytunnelpassword').read().decode('utf8').strip(\"\\n\")\n",
        "  !nohup npx --yes localtunnel --port 5001 > lt.log 2>&1 &\n",
        "  !sleep 8\n",
        "  print(\"=================\")\n",
        "  print(\"(LocalTunnel Results)\")\n",
        "  !cat lt.log\n",
        "  print(f\"Please open the above link, and input the password '{ltpw}'\\nYour KoboldCpp will start shortly...\")\n",
        "  print(\"=================\")\n",
        "  !sleep 10\n",
        "# !./koboldcpp_linux model.gguf --usecublas 0 mmq --chatcompletionsadapter AutoGuess --multiuser --gpulayers $Layers --contextsize $ContextSize --websearch --quiet --remotetunnel $FACommand $MPCommand $VCommand $SCommand $WCommand $TTSCommand $ECommand $SavGdriveCommand\n",
        "!./koboldcpp_linux {model_file_name} --usecublas 0 mmq --chatcompletionsadapter AutoGuess --multiuser --gpulayers $Layers --contextsize $ContextSize --websearch --quiet --remotetunnel $FACommand $MPCommand $VCommand $SCommand $WCommand $TTSCommand $ECommand $SavGdriveCommand\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}