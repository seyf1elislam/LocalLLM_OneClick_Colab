{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$</center>\n",
        " <center><h1>$\\color{#19ABEA}{\\text{Welcome there }}$</h1></center>\n",
        "<center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$</center>\n",
        "\n",
        "\n",
        "# UltimateOneClick_13BLLM_GGML (127 Models) with just 1 click\n",
        "## Hello there!\n",
        "\n",
        "* Here are 127 models (13B LLM ggml ), including the latest such as WizarldLLM1.2, LLAMA 2 chat...\n",
        "* Just choose a model, then run the cells.\n",
        "* More Quantized models will be added when they are released, as well as some GPTQ versions; simply check the repo again later.\n",
        "[$\\color{#19ABEA}{\\text{Click her to Check out what's new on Github.❤️}}$](https://github.com/seyf1elislam/LocalLLM_UltimateOneClick_Colab)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MjAmF39k1Nv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # $\\color{#19ABEA}{\\text{Run the model}}$ { display-mode: \"form\" }\n",
        "# %cd /content/text-generation-webui\n",
        "#@markdown # $\\color{#19ABEA}{\\text{Choose model :}}$\n",
        "\n",
        "choosed_model_name = \"TheBloke/Llama-2-13B-chat-GGML\" #@param ['TheBloke/13B-BlueMethod-GGML','TheBloke/13B-Chimera-GGML','TheBloke/13B-HyperMantis-GGML','TheBloke/13B-Legerdemain-L2-GGML','TheBloke/13B-Ouroboros-GGML','TheBloke/Airochronos-L2-13B-GGML','TheBloke/Airolima-Chronos-Grad-L2-13B-GGML','TheBloke/AlpacaCielo-13B-GGML','TheBloke/Baize-v2-13B-SuperHOT-8K-GGML','TheBloke/BigTranslate-13B-GGML','TheBloke/CAMEL-13B-Combined-Data-GGML','TheBloke/CAMEL-13B-Combined-Data-SuperHOT-8K-GGML','TheBloke/CAMEL-13B-Role-Playing-Data-GGML','TheBloke/CAMEL-13B-Role-Playing-Data-SuperHOT-8K-GGML','TheBloke/Camel-Platypus2-13B-GGML','TheBloke/Chronoboros-Grad-L2-13B-GGML','TheBloke/Chronohermes-Grad-L2-13B-GGML','TheBloke/Chronolima-Airo-Grad-L2-13B-GGML','TheBloke/Chronos-13B-SuperHOT-8K-GGML','TheBloke/Chronos-13B-v2-GGML','TheBloke/Chronos-Beluga-v2-13B-GGML','TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GGML','TheBloke/Chronos-Hermes-13B-v2-GGML','TheBloke/CodeUp-Alpha-13B-HF-GGML','TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML','TheBloke/Dolphin-Llama-13B-GGML','TheBloke/EverythingLM-13B-16K-GGML','TheBloke/Firefly-Llama2-13B-v1.2-GGML','TheBloke/GPT4All-13B-Snoozy-SuperHOT-8K-GGML','TheBloke/GPT4All-13B-snoozy-GGML','TheBloke/Hermes-LLongMA-2-13B-8K-GGML','TheBloke/Huginn-13B-GGML','TheBloke/Huginn-v3-13B-GGML','TheBloke/Karen_theEditor_13B-GGML','TheBloke/Kimiko-13B-GGML','TheBloke/Koala-13B-SuperHOT-8K-GGML','TheBloke/LLaMa-13B-GGML','TheBloke/Llama-2-13B-GGML','TheBloke/Llama-2-13B-chat-GGML','TheBloke/LongChat-13B-GGML','TheBloke/LosslessMegaCoder-Llama2-13B-Mini-GGML','TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GGML','TheBloke/Manticore-13B-Chat-Pyg-SuperHOT-8K-GGML','TheBloke/Manticore-13B-GGML','TheBloke/Manticore-13B-SuperHOT-8K-GGML','TheBloke/Minotaur-13B-fixed-SuperHOT-8K-GGML','TheBloke/MythoBoros-13B-GGML','TheBloke/MythoLogic-13B-GGML','TheBloke/MythoLogic-L2-13B-GGML','TheBloke/MythoMax-L2-13B-GGML','TheBloke/MythoMix-L2-13B-GGML','TheBloke/Nous-Hermes-13B-GGML','TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML','TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML','TheBloke/OpenOrca-Platypus2-13B-GGML','TheBloke/OpenOrca-Preview1-13B-GGML','TheBloke/OpenOrcaxOpenChat-Preview2-13B-GGML','TheBloke/Platypus2-13B-GGML','TheBloke/Project-Baize-v2-13B-GGML','TheBloke/Pygmalion-13B-SuperHOT-8K-GGML','TheBloke/Redmond-Puffin-13B-GGML','TheBloke/Robin-13B-v2-SuperHOT-8K-GGML','TheBloke/Samantha-13B-SuperHOT-8K-GGML','TheBloke/Scarlett-13B-GGML','TheBloke/Selfee-13B-GGML','TheBloke/Selfee-13B-GGML-DOI','TheBloke/Selfee-13B-SuperHOT-8K-GGML','TheBloke/Stable-Platypus2-13B-GGML','TheBloke/StableBeluga-13B-GGML','TheBloke/Tulu-13B-SuperHOT-8K-GGML','TheBloke/UltraLM-13B-GGML','TheBloke/Vicuna-13B-CoT-GGML','TheBloke/Vicuna-13B-v1.3-German-GGML','TheBloke/Vigogne-2-13B-Instruct-GGML','TheBloke/Vigogne-Instruct-13B-GGML','TheBloke/Wizard-Vicuna-13B-Uncensored-GGML','TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML','TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GGML','TheBloke/WizardLM-13B-Uncensored-GGML','TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML','TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML','TheBloke/WizardLM-13B-V1.0-Uncensored-GGML','TheBloke/WizardLM-13B-V1.1-GGML','TheBloke/WizardLM-13B-V1.2-GGML','TheBloke/WizardMath-13B-V1.0-GGML','TheBloke/airoboros-13B-1.1-GGML','TheBloke/airoboros-13B-gpt4-1.2-GGML','TheBloke/airoboros-13B-gpt4-1.3-GGML','TheBloke/airoboros-13B-gpt4-1.4-GGML','TheBloke/airoboros-13b-gpt4-GGML','TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGML','TheBloke/airoboros-l2-13b-gpt4-2.0-GGML','TheBloke/airoboros-l2-13b-gpt4-m2.0-GGML','TheBloke/based-13b-GGML','TheBloke/chronos-13B-GGML','TheBloke/chronos-hermes-13B-GGML','TheBloke/chronos-wizardlm-uc-scot-st-13B-GGML','TheBloke/gpt4-x-alpaca-13B-GGML','TheBloke/gpt4-x-vicuna-13B-GGML','TheBloke/guanaco-13B-GGML','TheBloke/h2ogpt-4096-llama2-13B-GGML','TheBloke/h2ogpt-4096-llama2-13B-chat-GGML','TheBloke/koala-13B-GGML','TheBloke/llama-13b-supercot-GGML','TheBloke/llama-2-13B-German-Assistant-v2-GGML','TheBloke/llama-2-13B-Guanaco-QLoRA-GGML','TheBloke/manticore-13b-chat-pyg-GGML','TheBloke/medalpaca-13B-GGML','TheBloke/minotaur-13B-GGML','TheBloke/minotaur-13B-fixed-GGML','TheBloke/open-llama-13b-open-instruct-GGML','TheBloke/orca_mini_13B-GGML','TheBloke/orca_mini_v2_13b-GGML','TheBloke/orca_mini_v3_13B-GGML','TheBloke/robin-13B-v2-GGML','TheBloke/samantha-1.1-llama-13B-GGML','TheBloke/samantha-13B-GGML','TheBloke/stable-vicuna-13B-GGML','TheBloke/tulu-13B-GGML','TheBloke/vicuna-13B-v1.5-16K-GGML','TheBloke/vicuna-13B-v1.5-GGML','TheBloke/vicuna-13b-1.1-GGML','TheBloke/vicuna-13b-v1.3.0-GGML','TheBloke/wizard-mega-13B-GGML','TheBloke/wizard-vicuna-13B-GGML','TheBloke/wizard-vicuna-13B-SuperHOT-8K-GGML','TheBloke/wizardLM-13B-1.0-GGML']{allow-input: true}\n",
        "use_short_menu = False #@param{type:\"boolean\"}\n",
        "if use_short_menu :choosed_model_name = \"TheBloke/WizardLM-13B-V1.2-GGML\" #@param [\"TheBloke/WizardLM-13B-V1.2-GGML\",\"TheBloke/Chronos-Beluga-v2-13B-GGML\",\"TheBloke/Llama-2-13B-chat-GGML\",\"TheBloke/MythoMax-L2-13B-GGML\",\"TheBloke/huginnv1.2-GGML\",\"TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML\",\"TheBloke/Kimiko-13B-GGML\"]\n",
        "#@markdown $\\color{#19ABEA}{\\text{Note :}}$ if u checked use_short_menu it will override the first choice\n",
        "#@markdown <center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$ </center>\n",
        "#@markdown\n",
        "#@markdown ##  Consider these options if you're struggling to decide what to run:\n",
        "#@markdown * TheBloke/WizardLM-13B-V1.2-GGML\n",
        "#@markdown * TheBloke/WizardLM-13B-V1.1-GGML\n",
        "#@markdown * TheBloke/wizard-vicuna-13B-GGML\n",
        "#@markdown * TheBloke/Llama-2-13B-chat-GGML\n",
        "#@markdown * TheBloke/MythoMax-L2-13B-GGML\n",
        "#@markdown * TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML\n",
        "#@markdown * TheBloke/Chronos-Beluga-v2-13B-GGML\n",
        "#@markdown <center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$</center>\n",
        "\n",
        "#@markdown # $\\color{#19ABEA}{\\text{Choose the Quantization}}$\n",
        "quantization =\"q5_K_M\" #@param ['q2_K', 'q3_K_L', 'q3_K_M', 'q3_K_S', 'q4_0', 'q4_1', 'q4_K_M', 'q4_K_S', 'q5_0', 'q5_1', 'q5_K_M', 'q5_K_S', 'q6_K', 'q8_0']\n",
        "#@markdown <center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$</center>\n",
        "\n",
        "#@markdown # $\\color{#19ABEA}{\\text{Check ur params and run this cell}}$\n",
        "#@markdown $\\color{#19ABEA}{\\text{Note :}}$ u dont need any of this just uncheck it and run the cell\n",
        "Public_API = False #@param{type:\"boolean\"}\n",
        "Listen = False #@param{type:\"boolean\"}\n",
        "# @markdown <b>note:</b> public_api include api param\n",
        "#@markdown if the u run choose the Api choice and it doesnt\n",
        "params=\" \"\n",
        "if Public_API :params +=\"--api --public-api \"\n",
        "if Listen :params +=\"--listen \"\n",
        "use_cutom_paramters = False #@param{type:\"boolean\"}\n",
        "custom_parameters=\"\"  #@param {type:\"string\"}\n",
        "if use_cutom_paramters :params +=\" \" + custom_parameters\n",
        "#@markdown  $\\color{#19ABEA}{\\text{Note :}}$  custom parameter will be added only if u check use_custom_parameters checkbox\n",
        "#@markdown </br></br>\n",
        "#@markdown  $\\color{#19ABEA}{\\text{How to know the model ready ? :}}$  when the model ready u will see a link like this in the cell output: \"Running on public URL: https://*******.gradio.live \" Run it\n",
        "#@markdown </br></br>\n",
        "#@markdown </br></br>\n",
        "\n",
        "#@markdown <center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$</center>\n",
        "#@markdown <center><h1>$\\color{#19ABEA}{\\text{Enjoy}}$</h1></center>\n",
        "#@markdown <center>$\\color{#19ABEA}{\\text{_____________________________________________________________________}}$</center>\n",
        "%cd /content\n",
        "!wget -O /content/models.py https://raw.githubusercontent.com/seyf1elislam/LocalLLM_UltimateOneClick_Colab/main/models.py\n",
        "\n",
        "from  models import models\n",
        "def get_model_url(model_name,quantzation_typ):\n",
        "    try :\n",
        "        file_name=models[model_name][quantzation_typ]\n",
        "        file_name=file_name[file_name.rindex(\"/\")+1:]\n",
        "        return f\"https://huggingface.co{models[model_name][quantzation_typ]}\",file_name;\n",
        "    except KeyError as e:\n",
        "        print(\"this model doest have this quntization \")\n",
        "        print(f\"here is list of quatization availabe for this model [{model_name}] \")\n",
        "        print(list(models[model_name].keys()) )\n",
        "        return -1,-1\n",
        "model_download_url ,model_file_name=get_model_url(choosed_model_name,quantization)\n",
        "if model_download_url != -1:\n",
        "  %cd /content\n",
        "  !apt-get -y install -qq aria2\n",
        "  # !git clone -b V20230720 https://github.com/Troyanovsky/text-generation-webui\n",
        "  !git clone -b V20230801 https://github.com/Troyanovsky/text-generation-webui\n",
        "  # !git clone https://github.com/oobabooga/text-generation-webui\n",
        "  %cd /content/text-generation-webui\n",
        "  !pip install -r requirements.txt\n",
        "  !pip install -U gradio==3.32.0\n",
        "  !pip install git+https://github.com/mnt4/flask-cloudflared\n",
        "  # !pip install flask_cloudflared\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "  from IPython.display import clear_output\n",
        "  clear_output(wait=True)\n",
        "  print(\"Done\")\n",
        "\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M $model_download_url -d /content/text-generation-webui/models/ -o $model_file_name\n",
        "  %cd /content/text-generation-webui\n",
        "  !python server.py --share --chat  --n-gpu-layers 200000 --model $model_file_name  $params"
      ],
      "metadata": {
        "id": "ycfTWOr9eSSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # $\\color{#19ABEA}{\\text{Run  gmml Model  From URL }}$ (this cell is independent of the previous cell | you dont need to run them both){ display-mode: \"form\" }\n",
        "# %cd /content/text-generation-webui\n",
        "\n",
        "#@markdown # $\\color{#19ABEA}{\\text{Enter URL :}}$\n",
        "\n",
        "model_bin_url = \"https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGML/resolve/main/wizardlm-13b-v1.2.ggmlv3.q5_K_M.bin\" # @param {type:\"string\"}\n",
        "#@markdown $\\color{#19ABEA}{\\text{Note:}}$ the link should be download link ends with .bin\n",
        "#@markdown $\\color{#19ABEA}{\\text{example :}}$ \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GGML/resolve/main/mythomax-l2-13b.ggmlv3.q3_K_L.bin\"\n",
        "\n",
        "#@markdown # $\\color{#19ABEA}{\\text{Check ur params and run this cell}}$\n",
        "#@markdown $\\color{#19ABEA}{\\text{Note :}}$ u dont need any of this just uncheck it and run the cell\n",
        "Public_API = True #@param{type:\"boolean\"}\n",
        "Listen = True #@param{type:\"boolean\"}\n",
        "# @markdown <b>note:</b> public_api include api param\n",
        "#@markdown if the u run choose the Api choice and it doesnt\n",
        "params=\" \"\n",
        "if Public_API :params +=\"--api --public-api \"\n",
        "if Listen :params +=\"--listen \"\n",
        "use_cutom_paramters = False #@param{type:\"boolean\"}\n",
        "custom_parameters=\"\"  #@param {type:\"string\"}\n",
        "if use_cutom_paramters :params +=\" \" + custom_parameters\n",
        "#@markdown <b>note:</b> custom parameter will be added only if u check use_custom_parameters checkbox\n",
        "if model_bin_url.startswith('https://huggingface.co') and model_bin_url.endswith(\".bin\"):\n",
        "  model_file_name=model_bin_url[model_bin_url.rindex(\"/\")+1:]\n",
        "  print(model_file_name)\n",
        "  %cd /content\n",
        "  !apt-get -y install -qq aria2\n",
        "  # !git clone -b V20230720 https://github.com/Troyanovsky/text-generation-webui\n",
        "  !git clone -b V20230801 https://github.com/Troyanovsky/text-generation-webui\n",
        "  # !git clone https://github.com/oobabooga/text-generation-webui\n",
        "  %cd /content/text-generation-webui\n",
        "  !pip install -r requirements.txt\n",
        "  !pip install -U gradio==3.32.0\n",
        "  !pip install git+https://github.com/mnt4/flask-cloudflared\n",
        "  # !pip install flask_cloudflared\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "  from IPython.display import clear_output\n",
        "  clear_output(wait=True)\n",
        "  print(\"Done\")\n",
        "\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M $model_bin_url -d /content/text-generation-webui/models/ -o $model_file_name\n",
        "  %cd /content/text-generation-webui\n",
        "  !python server.py --share --chat  --n-gpu-layers 200000 --model $model_file_name  $params\n"
      ],
      "metadata": {
        "id": "ZbUIAoVcC3Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # $\\color{#00FF00}{\\text{Run only command}}$:just in case u already run one of the previous cells and u dont want rerun it again { display-mode: \"form\" }\n",
        "!python server.py --share --chat  --n-gpu-layers 200000 --model $model_file_name  $params"
      ],
      "metadata": {
        "id": "6lC-w-rNCgQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}